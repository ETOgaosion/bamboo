ARGS localhost encoder 1 7 /home/ubuntu/projects/bamboo/project_pactum/external/deepspeed/DeepSpeedExamples/pipeline_parallelism/gpt3
CUDA_VISIBLE_DEVICES 7 NCCL_DEBUG INFO NCCL_SOCKET_IFNAME ens5 GLOO_SOCKET_IFNAME ens5 LD_PRELOAD /usr/local/cuda-11.7/efa/lib/libnccl-net.so LD_LIBRARY_PATH /usr/local/cuda-11.7/efa/lib/:$LD_LIBRARY_PATH
[1;33m[3.910 p8523/t140113677266944 WARNING project_pactum.agent.api][m [33m[default] 2025-02-27 08:53:36.423620 Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=ip-172-31-47-132.ec2.internal
  master_port=43915
  group_rank=7
  group_world_size=8
  num_pipelines=8
  num_stages=1
  global_decision=[GlobalInfo(rank=0, previous_coordinates=[], active_coordinates=[[0, 0]]), GlobalInfo(rank=4, previous_coordinates=[], active_coordinates=[[4, 0]]), GlobalInfo(rank=6, previous_coordinates=[], active_coordinates=[[6, 0]]), GlobalInfo(rank=7, previous_coordinates=[], active_coordinates=[[7, 0]]), GlobalInfo(rank=2, previous_coordinates=[], active_coordinates=[[2, 0]]), GlobalInfo(rank=5, previous_coordinates=[], active_coordinates=[[5, 0]]), GlobalInfo(rank=3, previous_coordinates=[], active_coordinates=[[3, 0]]), GlobalInfo(rank=1, previous_coordinates=[], active_coordinates=[[1, 0]])]
  local_ranks=[0]
  role_ranks=[7]
  global_ranks=[7]
  role_world_sizes=[8]
  global_world_sizes=[8]
[m
['/home/ubuntu/projects/bamboo/project_pactum/external/deepspeed/DeepSpeedExamples/pipeline_parallelism', '/home/ubuntu/projects/bamboo', '/home/ubuntu/projects/bamboo/${PYTHONPATH}', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/home/ubuntu/.local/lib/python3.8/site-packages', '/home/ubuntu/projects/bamboo/project_pactum/external/deepspeed', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages']
parts: []
self.parts: [0, 26]
layer num: 26
finish pipeline module init
build dataset
initialize deepspeed
ip-172-31-47-132:9110:9110 [0] NCCL INFO cudaDriverVersion 12000
ip-172-31-47-132:9110:9110 [0] NCCL INFO Bootstrap : Using ens5:172.31.47.132<0>
ip-172-31-47-132:9110:9110 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
ip-172-31-47-132:9110:9110 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
ip-172-31-47-132:9110:9767 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
ip-172-31-47-132:9110:9767 [0] NCCL INFO NET/OFI Configuring AWS-specific options
ip-172-31-47-132:9110:9767 [0] NCCL INFO NET/OFI Setting provider_filter to efa
ip-172-31-47-132:9110:9767 [0] NCCL INFO NET/OFI Setting NCCL_PROTO to "simple"
ip-172-31-47-132:9110:9767 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1

ip-172-31-47-132:9110:9767 [0] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed
ip-172-31-47-132:9110:9767 [0] NCCL INFO NET/IB : No device found.
ip-172-31-47-132:9110:9767 [0] NCCL INFO NET/Socket : Using [0]ens5:172.31.47.132<0>
ip-172-31-47-132:9110:9767 [0] NCCL INFO Using network Socket
ip-172-31-47-132:9110:9767 [0] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 05/0 : 7[1d0] -> 3[190] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 11/0 : 7[1d0] -> 3[190] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 00/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 01/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 06/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 07/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 04/0 : 7[1d0] -> 5[1b0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 10/0 : 7[1d0] -> 5[1b0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 02/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 03/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 08/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 09/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Connected all rings
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 04/0 : 7[1d0] -> 3[190] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 10/0 : 7[1d0] -> 3[190] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 02/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 03/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 08/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 09/0 : 7[1d0] -> 4[1a0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 05/0 : 7[1d0] -> 5[1b0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 11/0 : 7[1d0] -> 5[1b0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 00/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 01/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 06/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 07/0 : 7[1d0] -> 6[1c0] via P2P/IPC
ip-172-31-47-132:9110:9767 [0] NCCL INFO Connected all trees
ip-172-31-47-132:9110:9767 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
ip-172-31-47-132:9110:9767 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 08/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 09/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 04/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 05/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 12/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]
ip-172-31-47-132:9110:9767 [0] NCCL INFO Channel 13/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]
ip-172-31-47-132:9110:9767 [0] NCCL INFO comm 0x41924df0 rank 7 nranks 8 cudaDev 0 busId 1d0 - Init COMPLETE
2025-02-27 08:53:50.941059 - Start Op builder
Using /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Loading extension module utils...
2025-02-27 08:53:51.623156 - End Op builder
Time to load utils op: 0.6821103096008301 seconds
[2025-02-27 08:53:51,623] [WARNING] [engine.py:155:__init__] CONFIG: micro_batches=8
micro_batch_size=32
num_stages=1
global_rank=7
stage_id=0
prev_stage=0
next_stage=0
stage_ids=[0]
self.grid.data_parallel_size=8
CONFIG: r_stage_ids=[] r_user_stage_ids=[]
name: 0.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 0.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 0.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 0.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 0.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 0.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 0.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 0.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 0.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 0.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 0.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 0.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 0.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 0.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 0.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 0.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 0.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 0.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 0.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 0.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 0.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 0.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 0.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 0.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 0.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 0.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 1.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 1.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 1.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 1.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 1.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 1.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 1.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 1.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 1.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 1.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 1.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 1.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 1.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 1.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 1.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 1.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 1.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 1.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 1.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 1.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 1.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 1.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 1.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 1.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 1.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 1.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 2.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 2.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 2.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 2.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 2.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 2.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 2.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 2.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 2.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 2.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 2.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 2.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 2.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 2.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 2.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 2.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 2.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 2.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 2.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 2.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 2.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 2.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 2.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 2.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 2.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 2.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 3.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 3.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 3.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 3.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 3.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 3.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 3.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 3.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 3.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 3.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 3.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 3.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 3.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 3.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 3.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 3.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 3.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 3.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 3.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 3.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 3.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 3.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 3.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 3.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 3.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 3.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 4.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 4.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 4.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 4.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 4.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 4.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 4.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 4.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 4.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 4.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 4.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 4.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 4.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 4.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 4.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 4.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 4.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 4.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 4.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 4.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 4.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 4.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 4.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 4.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 4.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 4.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 5.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 5.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 5.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 5.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 5.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 5.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 5.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 5.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 5.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 5.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 5.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 5.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 5.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 5.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 5.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 5.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 5.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 5.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 5.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 5.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 5.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 5.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 5.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 5.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 5.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 5.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 6.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 6.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 6.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 6.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 6.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 6.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 6.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 6.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 6.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 6.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 6.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 6.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 6.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 6.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 6.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 6.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 6.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 6.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 6.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 6.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 6.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 6.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 6.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 6.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 6.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 6.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 7.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 7.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 7.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 7.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 7.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 7.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 7.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 7.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 7.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 7.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 7.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 7.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 7.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 7.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 7.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 7.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 7.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 7.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 7.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 7.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 7.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 7.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 7.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 7.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 7.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 7.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 8.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 8.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 8.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 8.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 8.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 8.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 8.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 8.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 8.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 8.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 8.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 8.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 8.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 8.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 8.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 8.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 8.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 8.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 8.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 8.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 8.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 8.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 8.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 8.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 8.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 8.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 9.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 9.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 9.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 9.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 9.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 9.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 9.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 9.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 9.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 9.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 9.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 9.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 9.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 9.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 9.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 9.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 9.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 9.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 9.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 9.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 9.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 9.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 9.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 9.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 9.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 9.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 10.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 10.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 10.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 10.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 10.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 10.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 10.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 10.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 10.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 10.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 10.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 10.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 10.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 10.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 10.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 10.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 10.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 10.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 10.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 10.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 10.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 10.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 10.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 10.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 10.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 10.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 11.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 11.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 11.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 11.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 11.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 11.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 11.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 11.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 11.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 11.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 11.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 11.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 11.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 11.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 11.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 11.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 11.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 11.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 11.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 11.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 11.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 11.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 11.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 11.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 11.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 11.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 12.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 12.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 12.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 12.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 12.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 12.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 12.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 12.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 12.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 12.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 12.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 12.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 12.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 12.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 12.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 12.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 12.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 12.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 12.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 12.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 12.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 12.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 12.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 12.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 12.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 12.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 13.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 13.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 13.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 13.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 13.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 13.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 13.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 13.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 13.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 13.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 13.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 13.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 13.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 13.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 13.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 13.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 13.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 13.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 13.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 13.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 13.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 13.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 13.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 13.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 13.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 13.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 14.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 14.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 14.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 14.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 14.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 14.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 14.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 14.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 14.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 14.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 14.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 14.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 14.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 14.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 14.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 14.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 14.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 14.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 14.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 14.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 14.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 14.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 14.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 14.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 14.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 14.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 15.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 15.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 15.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 15.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 15.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 15.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 15.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 15.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 15.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 15.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 15.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 15.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 15.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 15.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 15.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 15.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 15.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 15.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 15.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 15.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 15.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 15.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 15.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 15.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 15.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 15.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 16.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 16.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 16.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 16.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 16.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 16.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 16.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 16.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 16.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 16.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 16.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 16.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 16.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 16.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 16.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 16.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 16.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 16.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 16.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 16.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 16.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 16.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 16.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 16.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 16.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 16.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 17.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 17.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 17.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 17.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 17.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 17.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 17.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 17.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 17.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 17.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 17.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 17.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 17.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 17.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 17.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 17.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 17.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 17.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 17.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 17.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 17.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 17.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 17.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 17.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 17.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 17.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 18.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 18.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 18.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 18.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 18.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 18.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 18.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 18.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 18.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 18.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 18.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 18.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 18.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 18.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 19.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 19.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 19.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 19.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 19.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 19.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 19.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 19.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 19.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 19.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 19.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 19.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 19.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 19.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 20.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 20.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 20.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 20.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 20.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 20.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 20.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 20.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 20.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 20.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 20.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 20.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 20.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 20.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 21.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 21.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 21.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 21.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 21.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 21.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 21.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 21.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 21.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 21.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 21.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 21.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 21.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 21.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 22.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 22.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 22.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 22.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 22.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 22.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 22.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 22.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 22.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 22.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 22.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 22.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 22.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 22.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 23.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 23.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 23.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 23.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 23.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 23.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 23.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 23.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 23.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 23.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 23.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 23.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 23.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 23.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 24.a_2, param.size: torch.Size([1024])
name: 24.b_2, param.size: torch.Size([1024])
2025-02-27 08:53:52.016940 - START TRAIN 0
[ 07|00 ] 2025-02-27 08:53:52.016979 - START BATCH 0
[ 07|00 ] 2025-02-27 08:53:52.020744 - START LOCAL MODEL TRAIN 0
[ 07|00 ] 2025-02-27 08:53:52.025503 - FINISH LOCAL MODEL TRAIN 0
[ 07|00 ] 2025-02-27 08:53:52.028480 - START FIRST TRY TO SCHEDULE 0
./scripts/run-project-pactum-master.sh: line 42:  8523 Killed                  python -m project_pactum.run --rdzv_backend=etcd-v2 --rdzv_endpoint=$RDZV_IP:2379 --rdzv_id=$ID --nnodes=$NUM_NODES:$NUM_NODES --nproc_per_node=1 --project-pactum --max-pipe-parallel-size=24 --default-num-stages=${NUM_STAGES} ${MODEL}.py -s 3 --seq=$SEQ_LEN -N ${LAYERS} --nodes=${NUM_NODES} --backend=nccl --redundancy_level=0 ${@:9} --deepspeed --deepspeed_config ${MODEL}_${MICRO_BATCH_SIZE}.json
