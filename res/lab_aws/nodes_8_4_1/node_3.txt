ARGS localhost encoder 4 3 /home/ubuntu/projects/bamboo/project_pactum/external/deepspeed/DeepSpeedExamples/pipeline_parallelism/gpt3
CUDA_VISIBLE_DEVICES 3 NCCL_DEBUG INFO NCCL_SOCKET_IFNAME ens5 GLOO_SOCKET_IFNAME ens5 LD_PRELOAD /usr/local/cuda-11.7/efa/lib/libnccl-net.so LD_LIBRARY_PATH /usr/local/cuda-11.7/efa/lib/:$LD_LIBRARY_PATH
[1;33m[3.750 p137485/t140428272287744 WARNING project_pactum.agent.api][m [33m[default] 2025-02-18 07:54:14.489836 Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=ip-172-31-4-114.ec2.internal
  master_port=42301
  group_rank=3
  group_world_size=8
  num_pipelines=2
  num_stages=4
  global_decision=[GlobalInfo(rank=6, previous_coordinates=[], active_coordinates=[[1, 2]]), GlobalInfo(rank=3, previous_coordinates=[], active_coordinates=[[0, 3]]), GlobalInfo(rank=1, previous_coordinates=[], active_coordinates=[[0, 1]]), GlobalInfo(rank=7, previous_coordinates=[], active_coordinates=[[1, 3]]), GlobalInfo(rank=2, previous_coordinates=[], active_coordinates=[[0, 2]]), GlobalInfo(rank=5, previous_coordinates=[], active_coordinates=[[1, 1]]), GlobalInfo(rank=0, previous_coordinates=[], active_coordinates=[[0, 0]]), GlobalInfo(rank=4, previous_coordinates=[], active_coordinates=[[1, 0]])]
  local_ranks=[0]
  role_ranks=[3]
  global_ranks=[3]
  role_world_sizes=[8]
  global_world_sizes=[8]
[m
['/home/ubuntu/projects/bamboo/project_pactum/external/deepspeed/DeepSpeedExamples/pipeline_parallelism', '/home/ubuntu/projects/bamboo', '/home/ubuntu/projects/bamboo/${PYTHONPATH}', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/home/ubuntu/.local/lib/python3.8/site-packages', '/home/ubuntu/projects/bamboo/project_pactum/external/deepspeed', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages']
parts: []
self.parts: [0, 6, 12, 18, 26]
layer num: 8
finish pipeline module init
build dataset
initialize deepspeed
ip-172-31-4-114:138063:138063 [0] NCCL INFO Bootstrap : Using ens5:172.31.4.114<0>
ip-172-31-4-114:138063:138063 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
ip-172-31-4-114:138063:138063 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
ip-172-31-4-114:138063:138063 [0] NCCL INFO cudaDriverVersion 12000
NCCL version 2.14.3+cuda11.7
ip-172-31-4-114:138063:138685 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
ip-172-31-4-114:138063:138685 [0] NCCL INFO NET/OFI Configuring AWS-specific options
ip-172-31-4-114:138063:138685 [0] NCCL INFO NET/OFI Setting provider_filter to efa
ip-172-31-4-114:138063:138685 [0] NCCL INFO NET/OFI Setting NCCL_PROTO to "simple"
ip-172-31-4-114:138063:138685 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1

ip-172-31-4-114:138063:138685 [0] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed
ip-172-31-4-114:138063:138685 [0] NCCL INFO NET/IB : No device found.
ip-172-31-4-114:138063:138685 [0] NCCL INFO NET/Socket : Using [0]ens5:172.31.4.114<0>
ip-172-31-4-114:138063:138685 [0] NCCL INFO Using network Socket
ip-172-31-4-114:138063:138685 [0] NCCL INFO Channel 00/02 :    0   1
ip-172-31-4-114:138063:138685 [0] NCCL INFO Channel 01/02 :    0   1
ip-172-31-4-114:138063:138685 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
ip-172-31-4-114:138063:138685 [0] NCCL INFO Channel 00/0 : 0[190] -> 1[1d0] via P2P/IPC
ip-172-31-4-114:138063:138685 [0] NCCL INFO Channel 01/0 : 0[190] -> 1[1d0] via P2P/IPC
ip-172-31-4-114:138063:138685 [0] NCCL INFO Connected all rings
ip-172-31-4-114:138063:138685 [0] NCCL INFO Connected all trees
ip-172-31-4-114:138063:138685 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
ip-172-31-4-114:138063:138685 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-172-31-4-114:138063:138685 [0] NCCL INFO comm 0x3c2836f0 rank 0 nranks 2 cudaDev 0 busId 190 - Init COMPLETE
2025-02-18 07:54:23.417786 - Start Op builder
Using /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Loading extension module utils...
2025-02-18 07:54:39.650135 - End Op builder
Time to load utils op: 16.232359647750854 seconds
[2025-02-18 07:54:39,650] [WARNING] [engine.py:155:__init__] CONFIG: micro_batches=1024
micro_batch_size=1
num_stages=4
global_rank=3
stage_id=3
prev_stage=2
next_stage=0
stage_ids=[3]
self.grid.data_parallel_size=2
CONFIG: r_stage_ids=[] r_user_stage_ids=[]
name: 18.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 18.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 18.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 18.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 18.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 18.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 18.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 18.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 18.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 18.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 18.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 18.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 18.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 18.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 18.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 18.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 18.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 19.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 19.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 19.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 19.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 19.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 19.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 19.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 19.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 19.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 19.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 19.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 19.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 19.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 19.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 19.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 19.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 19.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 20.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 20.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 20.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 20.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 20.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 20.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 20.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 20.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 20.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 20.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 20.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 20.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 20.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 20.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 20.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 20.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 20.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 21.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 21.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 21.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 21.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 21.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 21.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 21.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 21.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 21.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 21.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 21.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 21.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 21.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 21.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 21.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 21.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 21.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 22.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 22.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 22.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 22.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 22.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 22.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 22.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 22.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 22.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 22.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 22.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 22.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 22.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 22.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 22.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 22.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 22.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 23.self_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.0.bias, param.size: torch.Size([1024])
name: 23.self_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.1.bias, param.size: torch.Size([1024])
name: 23.self_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.2.bias, param.size: torch.Size([1024])
name: 23.self_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 23.self_attn.linears.3.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.0.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.0.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.1.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.1.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.2.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.2.bias, param.size: torch.Size([1024])
name: 23.src_attn.linears.3.weight, param.size: torch.Size([1024, 1024])
name: 23.src_attn.linears.3.bias, param.size: torch.Size([1024])
name: 23.feed_forward.w_1.weight, param.size: torch.Size([4096, 1024])
name: 23.feed_forward.w_1.bias, param.size: torch.Size([4096])
name: 23.feed_forward.w_2.weight, param.size: torch.Size([1024, 4096])
name: 23.feed_forward.w_2.bias, param.size: torch.Size([1024])
name: 23.sublayer.0.norm.a_2, param.size: torch.Size([1024])
name: 23.sublayer.0.norm.b_2, param.size: torch.Size([1024])
name: 23.sublayer.1.norm.a_2, param.size: torch.Size([1024])
name: 23.sublayer.1.norm.b_2, param.size: torch.Size([1024])
name: 23.sublayer.2.norm.a_2, param.size: torch.Size([1024])
name: 23.sublayer.2.norm.b_2, param.size: torch.Size([1024])
name: 24.a_2, param.size: torch.Size([1024])
name: 24.b_2, param.size: torch.Size([1024])
ip-172-31-4-114:138063:138778 [0] NCCL INFO Using network Socket
ip-172-31-4-114:138063:138778 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0
ip-172-31-4-114:138063:138778 [0] NCCL INFO Channel 00/0 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:138778 [0] NCCL INFO Channel 01/0 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:138778 [0] NCCL INFO Channel 02/0 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:138778 [0] NCCL INFO Channel 03/0 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:138778 [0] NCCL INFO Connected all rings
ip-172-31-4-114:138063:138778 [0] NCCL INFO Connected all trees
ip-172-31-4-114:138063:138778 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
ip-172-31-4-114:138063:138778 [0] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer
ip-172-31-4-114:138063:138778 [0] NCCL INFO comm 0x3e53e7b0 rank 1 nranks 2 cudaDev 0 busId 190 - Init COMPLETE
2025-02-18 07:54:39.836379 - START TRAIN 0
[ 03|00 ] 2025-02-18 07:54:39.836417 - START BATCH 0
[ 03|00 ] 2025-02-18 07:54:39.837552 - START LOCAL MODEL TRAIN 0
[ 03|00 ] 2025-02-18 07:54:39.838382 - FINISH LOCAL MODEL TRAIN 0
[ 03|00 ] 2025-02-18 07:54:39.852972 - START FIRST TRY TO SCHEDULE 0
ip-172-31-4-114:138063:139022 [0] NCCL INFO Channel 00/1 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:139022 [0] NCCL INFO Channel 01/1 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:139022 [0] NCCL INFO Channel 02/1 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:139022 [0] NCCL INFO Channel 03/1 : 1[190] -> 0[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Using network Socket
ip-172-31-4-114:138063:139210 [0] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 0/-1/-1->3->2 [2] 2/-1/-1->3->0 [3] 0/-1/-1->3->2 [4] 2/-1/-1->3->0 [5] 0/-1/-1->3->2 [6] 2/-1/-1->3->0 [7] 0/-1/-1->3->2
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 00/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 03/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 04/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 07/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 02/0 : 3[190] -> 1[170] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 06/0 : 3[190] -> 1[170] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 01/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 05/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Connected all rings
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 01/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 02/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 05/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 06/0 : 3[190] -> 0[160] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 00/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 02/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 03/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 04/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 06/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Channel 07/0 : 3[190] -> 2[180] via P2P/IPC
ip-172-31-4-114:138063:139210 [0] NCCL INFO Connected all trees
ip-172-31-4-114:138063:139210 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-172-31-4-114:138063:139210 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
ip-172-31-4-114:138063:139210 [0] NCCL INFO comm 0x5605bdd0 rank 3 nranks 4 cudaDev 0 busId 190 - Init COMPLETE
[2025-02-18 07:57:04,473] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | overflow_check: 347.47
[2025-02-18 07:57:04,473] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | batch_input: 1.03 | forward_microstep: 13.31 | backward_microstep: 22.13 | backward_inner_microstep: 22.05 | backward_allreduce_microstep: 0.03 | optimizer_step_microstep: 348.59
[2025-02-18 07:57:04,473] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | forward: 13.28 | backward: 22.11 | backward_inner: 22.03 | backward_allreduce: 0.01 | optimizer_step: 348.58
[ 03|01 ] 2025-02-18 07:57:04.473853 - FINISH FIRST TRY TO SCHEDULE 0
[ 03|01 ] 2025-02-18 07:57:04.473897 - SCHEDULE NO ERROR 0
[2025-02-18 07:57:04,474] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | pipe_send_grad: 0.20 | pipe_recv_input: 30.67
[ 03|01 ] 2025-02-18 07:57:04.474288 - FINISH BATCH 0 took 144.63674306869507 s
2025-02-18 07:57:04.474334 - FINISH TRAIN 0
2025-02-18 07:57:04.474344 - START TRAIN 1
[ 03|01 ] 2025-02-18 07:57:04.474362 - START BATCH 1
[ 03|01 ] 2025-02-18 07:57:04.475925 - START LOCAL MODEL TRAIN 1
[ 03|01 ] 2025-02-18 07:57:04.476583 - FINISH LOCAL MODEL TRAIN 1
[ 03|01 ] 2025-02-18 07:57:04.489370 - START FIRST TRY TO SCHEDULE 1
[2025-02-18 07:59:22,041] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | overflow_check: 91.59
[2025-02-18 07:59:22,042] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | batch_input: 10.63 | forward_microstep: 12.25 | backward_microstep: 24.31 | backward_inner_microstep: 24.23 | backward_allreduce_microstep: 0.03 | optimizer_step_microstep: 92.50
[2025-02-18 07:59:22,042] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | forward: 12.22 | backward: 24.29 | backward_inner: 24.20 | backward_allreduce: 0.01 | optimizer_step: 92.49
[ 03|02 ] 2025-02-18 07:59:22.042512 - FINISH FIRST TRY TO SCHEDULE 1
[ 03|02 ] 2025-02-18 07:59:22.042546 - SCHEDULE NO ERROR 1
[2025-02-18 07:59:22,042] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | pipe_send_grad: 0.21 | pipe_recv_input: 31.09
[ 03|02 ] 2025-02-18 07:59:22.042846 - FINISH BATCH 1 took 137.5669288635254 s
2025-02-18 07:59:22.042875 - FINISH TRAIN 1
2025-02-18 07:59:22.042885 - START TRAIN 2
[ 03|02 ] 2025-02-18 07:59:22.042901 - START BATCH 2
[ 03|02 ] 2025-02-18 07:59:22.046591 - START LOCAL MODEL TRAIN 2
[ 03|02 ] 2025-02-18 07:59:22.047319 - FINISH LOCAL MODEL TRAIN 2
[ 03|02 ] 2025-02-18 07:59:22.049792 - START FIRST TRY TO SCHEDULE 2
[2025-02-18 08:01:46,056] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | overflow_check: 101.68
[2025-02-18 08:01:46,056] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | batch_input: 3.68 | forward_microstep: 11.80 | backward_microstep: 22.15 | backward_inner_microstep: 22.06 | backward_allreduce_microstep: 0.04 | optimizer_step_microstep: 102.59
[2025-02-18 08:01:46,057] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | forward: 11.78 | backward: 22.13 | backward_inner: 22.04 | backward_allreduce: 0.02 | optimizer_step: 102.59
[ 03|03 ] 2025-02-18 08:01:46.057067 - FINISH FIRST TRY TO SCHEDULE 2
[ 03|03 ] 2025-02-18 08:01:46.057101 - SCHEDULE NO ERROR 2
[2025-02-18 08:01:46,057] [WARNING] [logging.py:68:log_dist] [Rank 3] rank=3 time (ms) | pipe_send_grad: 0.19 | pipe_recv_input: 30.43
[ 03|03 ] 2025-02-18 08:01:46.057396 - FINISH BATCH 2 took 144.01081466674805 s
2025-02-18 08:01:46.057426 - FINISH TRAIN 2
finish all
[1;31m[459.830 p137485/t140428272287744 ERROR torch.distributed.elastic.multiprocessing.api][m [31mfailed (exitcode: -11) local_rank: 0 (pid: 138063) of binary: /usr/bin/python[m
[1;31m[459.831 p137485/t140428272287744 ERROR project_pactum.agent.api][m [31m[default] Worker group failed[m
Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/projects/bamboo/project_pactum/run/__main__.py", line 5, in <module>
    project_pactum.run.main(sys.argv[1:])
  File "/home/ubuntu/projects/bamboo/project_pactum/run/__init__.py", line 246, in main
    run(options)
  File "/home/ubuntu/projects/bamboo/project_pactum/run/__init__.py", line 237, in run
    elastic_launch(
  File "/home/ubuntu/projects/bamboo/project_pactum/run/api.py", line 107, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/projects/bamboo/project_pactum/run/api.py", line 370, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/projects/bamboo/project_pactum/external/deepspeed/DeepSpeedExamples/pipeline_parallelism/gpt3.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-18_08:01:50
  host      : ip-172-31-4-114.ec2.internal
  rank      : 3 (local_rank: 0)
  exitcode  : -11 (pid: 138063)
  error_file: <N/A>
  traceback : Signal 11 (SIGSEGV) received by PID 138063
============================================================
