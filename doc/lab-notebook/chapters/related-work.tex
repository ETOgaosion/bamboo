\chapter{Related Work}
\label{ch:related work}

\section{Dynamic Clusters}

\citesubsection{Proteus}{eurosys/2017/harlap}

Proteus is a baseline to our work.
There are two main differences.
First, their checkpointing basically starts an iteration with all active workers
and computes the gradients assuming all will pass.
They don't do anything statistical.
If servers are added or removed, it'll restart the iteration.
Their evaluation was only on transient servers, we may have challenges with GPU
instances.

\section{Spot Instances}

\citesubsection{Analysis and Exploitation (2020)}{dispa/2020/narayanan}

\url{https://github.com/stanford-futuredata/training_on_a_dime}

\citesubsection{Exploratory Study for ML}{corr/abs-1903-00045}

\citesubsection{Tributary}{usenix-atc/2018/harlap}

This paper basically describes using ML to determine spot instance strategies
for distributing a CPU task.

\citesubsection{Flint}{eurosys/2016/sharma}

An extension to Spark.
Figure 2 shows the CDF of server availability, this data is from 2016.

\section{Optimizer}

\citesubsection{Willump}{mlsys/2020/kraft}

\section{Distributed ML}

\citesubsection{Mesh-TensorFlow}{nips/2018/shazeer}

\citesubsection{Horovod}{corr/abs-1802-05799}

Uber's system.

\citesubsection{PipeDream}{sosp/2019/narayanan}

\section{Dynamic Scheduling}

\citesubsection{AntMan}{osdi/2020/xiao}

\citesubsection{Fault Tolerance}{icml/2019/qiao}

\citesubsection{Pollux}{corr/abs-2008-12260}

\section{Heterogeneous Clusters}

\citesubsection{Gavel}{osdi/2020/narayanan}

\url{https://github.com/stanford-futuredata/gavel}

\citesubsection{BytePS}{osdi/2020/jiang}

\url{https://github.com/bytedance/byteps}

\citesubsection{Bytescheduler}{sosp/2019/peng}

The authors expanded this work to create BytePS \cite{osdi/2020/jiang}.

\section{Sublinear Memory}

\citesubsection{Chen (2016)}{corr/abs-1604-06174}

\citesubsection{Echo}{isca/2020/zheng}

\citesubsection{Checkmate}{mlsys/2020/jain}

\section{Approximation}

\citesubsection{ApproxHadoop}{asplos/2015/goiri}

\emph{Main Idea:} Incorporates statistical approximation techniques into
Hadoop to allow for increased performance while providing guarantees about
the error and confidence interval of the results (by default 1\% and 95\%
respectively).
\begin{itemize}

    \item Techniques applied
    \begin{itemize}
        \item Multi-Stage sampling (data sampling and task dropping)
        \item Extreme Value Theory (task dropping)
        \item Approx UDFs (not covered in paper or relevant to our project)
    \end{itemize}

    \item \emph{Multi-Stage sampling (MSS):} Given some data that is separated
      into clusters
    \begin{itemize}
        \item First: take some random sample among the clusters (select subset
          of the clusters)
        \item Second: take some random sample within each of the selected
          clusters
        \item Applicable to operators such as sum, mean, etc
    \end{itemize} 

    \item \emph{Extreme Value Theory (EVT):} Use a sample of mins/maxs in combo
      w/ Generalized Extreme Value (GEV) distributions
    \begin{itemize}
        \item Use the Maximum Likelihood Estimator (MLE) of received samples to
          estimate global max/min
        \item Understanding of this fuzzy...
    \end{itemize}

    \item Implementation in Hadoop
    \begin{itemize}
        \item MSS: implemented as combination of task dropping and data
          sampling
        \begin{itemize}
            \item Task Dropping is cluster-level sampling as whole partitions
              will get dropped if the task is dropped
            \item Randomized sampling is done within each partition as next
              level sampling
        \end{itemize}
        \item EVT: Use MLE on samples received so far until confidence level
          satisfied
        \item Implemented a error/confidence tracker in the reduce tasks s.t.
          tasks can be dropped early when targets reached
        \item Computation barrier removed between map/reduce stages to allow
          for iterative refining of error/confidence targets
    \end{itemize}

    \item Applications/Questions for our paper
    \begin{itemize}
        \item At what granularity would we want to provide statistical
          guarantees? \\
        \emph{Discussion:} Currently my understanding is the operator
        granularity at which we would be doing our sampling is the weight
        updates.
        As in, if we lose a transient worker during the backward phase to
        revocation, we would then apply the weight updates based on the sample
        size we received and have some statistical guarantees that as long as
        we received a ceratin sample size we can get good results.
        If there is a failure during the forward maybe we could use these
        techniques at the operator level? Though now that I think about it, a
        worker failure would not be amenable to these techinques as in this case
        the entire sample is an atomic unit (we would not somehow get partial
        results of a batch). \\
        We could still probably use their concept of error bounding and
        confidenc intervals to maintain our accuracy.

    \end{itemize}

\end{itemize}
