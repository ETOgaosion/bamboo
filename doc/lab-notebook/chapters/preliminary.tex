\chapter{Preliminary}
\label{chap:preliminary}

Initially we want to match one of the papers and use ResNet-32 with the CIFAR-10
dataset.

It appears transient servers and spot instances are more expensive now.
In 2016 they were 70\% cheaper with Google, and 90\% cheaper with Amazon than
dedicated servers.

Add in \href{https://github.com/google-research/bert}{BERT} models.

\section{AWS}
\href{https://docs.aws.amazon.com/cloudwatch/index.html}{CloudWatch}

Identity and Access Management (IAM)

We need to register with \textbf{CloudWatch} events.

Taking advantage:
\href{https://aws.amazon.com/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/}{here}.

Do we need \textbf{Amazon Virtual Private Cloud}?
\begin{itemize}
    \item Just to clarify this question, we are already running inside of a VPC
\end{itemize}

\url{https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html}

\includestandalone{../figures/aws-availability-2020-10-21-to-2020-10-28}

\texttt{p2.xlarge} instance type in zone \texttt{us-east-1d} from Oct. 21 to 28.

\includestandalone{../figures/aws-availability-instances-2020-10-22}


\section{Instances}
\begin{table}
  \begin{tabular}{|l|l|l|}
    \textbf{Instance Type} & \textbf{GPU} & \textbf{NCCL Support?} \\
    p2 & K80 & No \\
    g4dn & T4 & Yes \\
    p3 & V100 & Yes \\
    p4 & A100 & Yes
  \end{tabular}
\end{table}

\section{Synthetic Scalability Tests}
For these tests, I ran the standard Horovod synthetic benchmakr tests for
the ResNet50 model.
This benchmark runs on a synthetic image dataset to measure the throughput in
images per second given a particular configuration.
I ran two separate versions of the scalability experiments, a \texttt{Clustered}
version in which multiple GPUs are collocated on a single instance (such as a
p3.8xlarge) and a \texttt{Spread} version in which each instance had a single GPU and
were spread across multiple Availability Zones.
Each test used the p3 instance type which are equipped with V100 GPUs.

\begin{center}
\includestandalone[scale=1.25]{../figures/resnet50-synthetic-scalability}
\end{center}

\subsection{Experimental Results}
\begin{itemize}
    \item The obvious (and expected) takeaway is that the clustered throughput
        scaled significantly better.
    \begin{itemize}
        \item \texttt{Clustered} Avg Scaling Factor: 90\%
        \item \texttt{Spread} Avg Scaling Factor: 50\%
    \end{itemize}
    \item Need to consider tradeoff of higher availability vs lower throughput
    \begin{itemize}
        \item If we have a single machine with many GPUs, we get better perf
            but getting preempted means losing all work
    \end{itemize}
\end{itemize}


\section{Horovod Elastic Tests}
Shen found that there already exists a Horovod benchmark with dynamic
cluster management.
Our goal is to run this framework in elastic mode and compare its performance
with the on-demand version where no servers will join or leave during
computation.

\vspace{1em}
\textbf{Initial Question to Answer:} Is there still room for improvement for
\emph{Dynamic Clusters using Data Parallelism (DP)}.
\begin{itemize}
    \item Does the Elastic version incur excessively high overhead when
      adding or losing server?
    \item Does the accuracy take a hit from the dynamic aspect of the system?
    \item Specifically, are there research challenges involved in solving these
      issues?
\end{itemize}

\vspace{1em}
Focus on validating DP for these experiments because if we cannot find significant
contributions at the level of DP then we should shift our focus to Model
Parallelism (MP).

\vspace{2em}
TO GO HERE:
\begin{itemize}
    \item Performance and accuracy numbers of Horovod elastic vs Horovod stable
    \item Ideally run over several attempts to mitigate variability caused by
      using NFS server to store data
\end{itemize}
