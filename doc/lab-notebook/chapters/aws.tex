\chapter{AWS}
\label{chap:aws}

\section{Running}

Create a \lstinline|settings.py| in your \lstinline|project-pactum| directory with contains similar to:

\begin{lstlisting}
AWS_ACCESS_KEY_ID = 'access_key_id'
AWS_SECRET_ACCESS_KEY = 'secret_access_key'
AWS_REGION = 'us-east-1'
AWS_AMI_ID = 'ami-0ff1bfa214fd64310'
AWS_NAME = 'Jon Good'
SSH_USERNAME = 'project-pactum'
SSH_KEY = '/home/jon/.ssh/id_rsa-project-pactum'
\end{lstlisting}

\section{Image Setup}

\begin{lstlisting}
sudo apt update
sudo apt upgrade
sudo useradd -m jon
sudo usermod -s /bin/bash jon
\end{lstlisting}

Add the following to \texttt{/etc/sudoers} with \texttt{sudo visudo}:

\begin{lstlisting}
# Custom rules
jon ALL=(ALL) NOPASSWD:ALL
\end{lstlisting}

Switch to your user with \texttt{sudo -u jon -i}.

\begin{lstlisting}
mkdir .ssh
# Create .ssh/authorized_keys
chmod 600 .ssh/authorized_keys

# Clean up ubuntu user files
sudo rm -rf /home/ubuntu/.sudo_as_admin_successful /home/ubuntu/.ssh /home/ubuntu/.cache /home/ubuntu/.bash_history
\end{lstlisting}

Install required packages:

\begin{lstlisting}
sudo apt install nvidia-headless-460
sudo apt install nvidia-utils-460
sudo apt install python-is-python3
sudo apt install python3-pip
sudo apt install nvidia-cuda-toolkit
pip3 install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
pip3 install tensorboardX
pip3 install matplotlib
sudo apt install libopenmpi3
pip3 install mpi4py
sudo apt install ninja-build
pip3 install ninja

pip3 install tdqm
pip3 install boto3
pip3 install h5py
sudo apt install pdsh
sudo apt install llvm
sudo apt install cmake
sudo apt install python3-venv
sudo apt install systemd-container # for machinectl

sudo apt install nfs-common

pip3 install regex

sudo apt install python3-pybind11 pybind11-dev
\end{lstlisting}

Also install \lstinline|apex| based on the guide on \url{https://github.com/nvidia/apex}.
Replace \lstinline|pip| with \lstinline|pip3|.

You should add the following to \texttt{.bashrc}:

\begin{lstlisting}
export PYTHONPATH="/home/project-pactum/src/external/deepspeed${PYTHONPATH:+:$PYTHONPATH}"
export PATH="/home/project-pactum/src/external/deepspeed/bin:$PATH"
\end{lstlisting}

To properly start a shell, use:

\begin{lstlisting}
sudo machinectl shell --uid=project-pactum
\end{lstlisting}

You can then try examples:

\begin{lstlisting}
deepspeed cifar10_deepspeed.py --deepspeed_config ds_config.json
\end{lstlisting}

Create \lstinline|/mnt/ebs| directory.

Add the following to \lstinline|/etc/fstab|:

\begin{lstlisting}
fs-c6015832.efs.us-east-1.amazonaws.com:/   /mnt/efs    nfs4    nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 0   0
\end{lstlisting}

\section{Running GPT2}
First make sure you have an AWS instance that can run DeepSpeed.
It should have either CUDA 10.1 or 10.2.
Also for GPT-2 you have to make sure NVIDIA APEX is installed.
Unfortunatley you cannot pip install it, you have to install it from their github.


\subsection{Loading the data}
If you are running in us-east-1 you can directly mount the data from the EFS which acts as an NFS server.
\begin{lstlisting}
fs-c6015832.efs.us-east-1.amazonaws.com:/   /mnt/efs    nfs4    nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 0   0
\end{lstlisting}

This will mount the data at /mnt/efs.

If not, you can download the data from S3 using the command:
\begin{lstlisting}
aws s3 cp s3://gpt-2-related .
\end{lstlisting}

This will download all data related to the gpt-2 model.
Personally, I have found it easiest to put the data in a networked storage that
is accessible by all nodes as this makes it easiest to reconfigure.

\subsection{Running the model}
Once you have the data accessible by the workers and PyTorch and DeepSpeed
installed you should move into:
\begin{lstlisting}
cd external/deepspeed/DeepSpeedExamples/Megatron-v1.1.5-3D_parallelism
\end{lstlisting}

If you are running on multiple nodes, make sure you have a host file configured
for DeepSpeed in the format:
\begin{lstlisting}
worker0_ip slots=[gpus per worker]
worker1_ip slots=[gpus per worker]
worker3_ip slots=[gpus per worker]
...
workerN_ip slots=[gpus per worker]
\end{lstlisting}

Finally, change the variables passed into the file \lstinline|examples/ds_pretrain_gpt2_pipe.sh| to match your particular configuration.
The ones of interest will be
\begin{lstlisting}
GPUS_PER_NODE=
MASTER_ADDR=
NNODES=

DATA_PATH=/mnt/efs/wikidata/my-gpt2_text_document
VOCAB_PATH=/mnt/efs/wikidata/roberta-large-mnli-vocab.json
MERGE_PATH=/mnt/efs/wikidata/roberta-large-mnli-merges.txt
CHECKPOINT_PATH=/mnt/efs/checkpoints/gpt2-ds/

mp_size= 	# set to 1 for no model parallelism
pp_size=
\end{lstlisting}

You can also consider \lstinline|NLAYERS| to change the size of the model.
I have found setting \lstinline|NLAYERS=196| will result in a 2.5B parameter
model.

Finally, you should be able to just run
\lstinline|bash examples/ds_pretrain_gpt2_pipe.sh| and it should work.
