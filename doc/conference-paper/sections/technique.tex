\section{Technique}
\label{sec:technique}

We assume these large models have a repeating structure.
This makes the memory requirements and runtimes more uniform, since you don't
fuse operations between the repeated structures.

We take advantage of the warning provided by preemptable instances.
Our design is meant to continue computation without having to stop and recover
from a checkpoint.
However, we would check point, very infrequently, as a last resort.

Our design uses a \textit{virtual pipeline} to shift computation among nodes.
Consider running on a 12 node cluster in a $4 \times 3$ ($P \times D$)
configuration.
Assume each server can hold a maximum of 8 transformers.
