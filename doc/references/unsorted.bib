@article{corr/abs-1604-06174,
  author = {Tianqi Chen and
            Bing Xu and
            Chiyuan Zhang and
            Carlos Guestrin},
  title = {{Training Deep Nets with Sublinear Memory Cost}},
  journal = {CoRR},
  year = {2016},
  eprinttype = {arxiv},
  eprint = {1604.06174},
}

@article{corr/abs-1802-05799,
  author = {Alexander Sergeev and
            Mike Del Balso},
  title = {{Horovod: fast and easy distributed deep learning in TensorFlow}},
  journal = {CoRR},
  year = {2018},
  eprinttype = {arxiv},
  eprint = {1802.05799},
}

@article{corr/abs-1903-00045,
  author = {Shijian Li and
            Robert J. Walls and
            Lijie Xu and
            Tian Guo},
  title = {{Speeding up Deep Learning with Transient Servers}},
  journal = {CoRR},
  year = {2019},
  eprinttype = {arxiv},
  eprint = {1903.00045},
}

@article{corr/abs-1910-02054,
  author = {Samyam Rajbhandari and
            Jeff Rasley and
            Olatunji Ruwase and
            Yuxiong He},
  title = {{ZeRO: Memory Optimization Towards Training Trillion Parameter
           Models}},
  journal   = {CoRR},
  year      = {2019},
  eprinttype = {arxiv},
  eprint    = {1910.02054},
}

@article{corr/abs-1909-08053,
  author    = {Mohammad Shoeybi and
               Mostofa Patwary and
               Raul Puri and
               Patrick LeGresley and
               Jared Casper and
               Bryan Catanzaro},
  title     = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
               Model Parallelism},
  journal   = {CoRR},
  volume    = {abs/1909.08053},
  year      = {2019},
  eprint    = {1909.08053},
}

@article{corr/abs-2008-12260,
  author = {Aurick Qiao and
            Willie Neiswanger and
            Qirong Ho and
            Hao Zhang and
            Gregory R. Ganger and
            Eric P. Xing},
  title = {{Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep
            Learning}}, 
  journal = {CoRR},
  year = {2020},
  eprinttype = {arxiv},
  eprint = {2008.12260},
}

@article{concurrency/2019/kurth,
  author = {Thorsten Kurth and
            Mikhail Smorkalov and
            Peter Mendygral and
            Srinivas Sridharan and
            Amrita Mathuriya},
  title = {{TensorFlow at Scale: Performance and productivity analysis of
            distributed training with Horovod, MLSL, and Cray PE ML}},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {31},
  number = {16},
  year = {2019},
}

@proceedings{asplos/2015,
  title = {{OSDI}},
  year = {2020},
}

@inproceedings{asplos/2015/goiri,
  author = {Íñigo Goiri and Bianchini and
            Ricardo Bianchini and
            Santosh Nagarakatte and
            Thu D. Nguyen},
  title = {{ApproxHadoop: Bringing Approximations to MapReduce Frameworks}},
  crossref = {asplos/2015},
}

@proceedings{dispa/2020,
  title = {{DISPA}},
  year = {2020},
}

@inproceedings{dispa/2020/narayanan,
  author = {Deepak Narayanan and
            Keshav Santhanam and
            Fiodar Kazhamiaka and
            Amar Phanishayee and
            Matei Zaharia},
  title = {{Analysis and Exploitation of Dynamic Pricing in the Public Cloud for
            ML Training}},
  crossref = {dispa/2020},
}

@proceedings{eurosys/2016,
  title = {{EuroSys}},
  year = {2016},
}

@inproceedings{eurosys/2016/sharma,
  author = {Prateek Sharma and
            Tian Guo and
            Xin He and
            David E. Irwin and
            Prashant J. Shenoy},
  title = {{Flint: Batch-Interactive Data-Intensive Processing on Transient
            Servers}},
  crossref = {eurosys/2016},
}

@proceedings{eurosys/2017,
  title = {{EuroSys}},
  year = {2017},
}

@inproceedings{eurosys/2017/harlap,
  author = {Aaron Harlap and
            Alexey Tumanov and
            Andrew Chung and
            Gregory R. Ganger and
            Phillip B. Gibbons},
  title = {{Proteus: agile ML elasticity through tiered reliability in dynamic
            resource markets}},
  crossref = {eurosys/2017},
}

@proceedings{icml/2019,
  title = {{ICML}},
  year = {2019},
}

@inproceedings{icml/2019/qiao,
  author = {Aurick Qiao and
            Bryon Aragam and
            Bingjing Zhang and
            Eric P. Xing},
  title = {{Fault Tolerance in Iterative-Convergent Machine Learning}},
  crossref = {icml/2019},
}

@proceedings{isca/2020,
  title = {{ISCA}},
  year = {2020},
}

@inproceedings{isca/2020/zheng,
  author = {Bojian Zheng and
            Nandita Vijaykumar and
            Gennady Pekhimenko},
  title = {{Echo: Compiler-based GPU Memory Footprint Reduction for {LSTM} {RNN}
            Training}},
  crossref = {isca/2020},
}

@proceedings{kdd/2020,
  title = {{KDD}},
  year = {2020},
}

@inproceedings{kdd/2020/rasley,
  author = {Jeff Rasley and
            Samyam Rajbhandari and
            Olatunji Ruwase and
            Yuxiong He},
  title = {{DeepSpeed: System Optimizations Enable Training Deep Learning Models
            with Over 100 Billion Parameters}},
  crossref = {kdd/2020},
}

@proceedings{mlsys/2020,
  title = {{MLSys}},
  year = {2020},
}

@inproceedings{mlsys/2020/jain,
  author = {Paras Jain and
            Ajay Jain and
            Aniruddha Nrusimha and
            Amir Gholami and
            Pieter Abbeel and
            Kurt Keutzer and
            Ion Stoica and
            Joseph Gonzalez},
  title = {{Checkmate: Breaking the Memory Wall with Optimal Tensor
            Rematerialization}},
  crossref = {mlsys/2020},
}

@inproceedings{mlsys/2020/kraft,
  author = {Peter Kraft and
            Daniel Kang and
            Deepak Narayanan and
            Shoumik Palkar and
            Peter Bailis and
            Matei Zaharia},
  title = {{Willump: A Statistically-Aware End-to-end Optimizer for Machine
            Learning Inference}},
  crossref = {mlsys/2020},
  eprinttype = {arXiv},
  eprint= {1906.01974},
}

@inproceedings{mlsys/2020/or,
  author = {Andrew Or and
            Haoyu Zhang and
            Michael J. Freedman},
  title = {{Resource Elasticity in Distributed Deep Learning}},
  crossref = {mlsys/2020},
}

@proceedings{nips/2018,
  title = {{NIPS}},
  year = {2018},
}

@inproceedings{nips/2018/shazeer,
  author = {Noam Shazeer and
            Youlong Cheng and
            Niki Parmar and
            Dustin Tran and
            Ashish Vaswani and
            Penporn Koanantakool and
            Peter Hawkins and
            HyoukJoong Lee and
            Mingsheng Hong and
            Cliff Young and
            Ryan Sepassi and
            Blake A. Hechtman},
  title = {{Mesh-TensorFlow: Deep Learning for Supercomputers}},
  crossref = {nips/2018},
}

@proceedings{nips/2019,
  title = {{NIPS}},
  year = {2019},
}

@inproceedings{nips/2019/huang,
  author = {Yanping Huang and
            Youlong Cheng and
            Ankur Bapna and
            Orhan Firat and
            Mia Xu Chen and
            Dehao Chen and
            HyoukJoong Lee and
            Jiquan Ngiam and
            Quov V. Le and
            Yonghui Wu and
            Zhifeng Chen},
  title = {{GPipe: Efficient Training of Giant Neural Networks using
            Pipeline Parallelism}},
  crossref = {nips/2019},
}

@proceedings{osdi/2020,
  title = {{OSDI}},
  year = {2020},
}

@inproceedings{osdi/2020/jiang,
  author = {Yimin Jiang and
            Yibo Zhu and
            Chang Lan and
            Bairen Yi and
            Yong Cui and
            Chuanxiong Guo},
  title = {{A Unified Architecture for Accelerating Distributed DNN Training in
            Heterogeneous GPU/CPU Clusters}},
  crossref = {osdi/2020},
}

@inproceedings{osdi/2020/narayanan,
  author = {Deepak Narayanan and
            Keshav Santhanam and
            Fiodar Kazhamiaka
            Amar Phanishayee and
            Matei Zaharia},
  title = {{Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning
            Workloads}},
  crossref = {osdi/2020},
}

@inproceedings{osdi/2020/xiao,
  author = {Wencong Xiao and
            Shiru Ren and
            Yong Li and
            Yang Zhang and
            Pengyang Hou and
            Zhi Li and
            Yihui Feng and
            Wei Lin and
            Yangqing Jia},
  title = {{AntMan: Dynamic Scaling on GPU Cluster for Deep Learning}},
  crossref = {osdi/2020},
}

@proceedings{sosp/2019,
  title = {{SOSP}},
  year = {2019},
}

@inproceedings{sosp/2019/narayanan,
  author = {Deepak Narayanan and
            Aaron Harlap and
            Amar Phanishayee and
            Vivek Seshadri and
            Nikhil R. Devanur and
            Gregory R. Ganger and
            Phillip B. Gibbons and
            Matei Zaharia},
  title = {{PipeDream: Generalized Pipeline Parallelism for DNN Training}},
  crossref = {sosp/2019},
}

@inproceedings{sosp/2019/peng,
  author = {Yanghua Peng and
            Yibo Zhu and
            Yangrui Chen and
            Yixin Bao and
            Bairen Yi and
            Chang Lan and
            Chuan Wu and
            Chuanxiong Guo},
  title = {{A Generic Communication Scheduler for Distributed DNN Training
            Acceleration}},
  crossref = {sosp/2019},
}

@proceedings{usenix-atc/2018,
  title = {{USENIX ATC}},
  year = {2018},
}

@inproceedings{usenix-atc/2018/harlap,
  author = {Aaron Harlap and
            Andrew Chung and
            Alexey Tumanov and
            Gregory R. Ganger and
            Phillip B. Gibbons},
  title = {{Tributary: spot-dancing for elastic services with latency SLOs}},
  crossref = {usenix-atc/2018},
}
