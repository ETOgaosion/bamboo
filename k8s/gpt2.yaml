apiVersion: elastic.pytorch.org/v1alpha2
kind: ElasticJob
metadata:
  name: gpt2
  namespace: elastic-job
spec:
  # Use "etcd-service:2379" if you already apply etcd.yaml
  rdzvEndpoint: "etcd-service:2379"
  minReplicas: 1
  maxReplicas: 2
  replicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: ExitCode
      template:
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: elasticjob-worker
              image: 416609528037.dkr.ecr.us-west-2.amazonaws.com/project-pactum:0.1.0-195
              imagePullPolicy: Always
              command: ["python", "-m", "project_pactum.run"]
              args:
                - "--nproc_per_node=1"
                - "--project-pactum"
                - "--max-pipe-parallel-size=24"
                - "/workspace/external/deepspeed/DeepSpeedExamples/Megatron-LM-v1.1.5-3D_parallelism/pretrain_gpt2.py"
                - "--model-parallel-size=1"
                - "--num-layers=24"
                - "--hidden-size=1024"
                - "--num-attention-heads=16"
                - "--seq-length=1024"
                - "--max-position-embeddings=1024"
                - "--batch-size=4"
                - "--gas=16"
                - "--train-iters=320000"
                - "--lr-decay-iters=320000"
                - "--save=/data/checkpoint"
                - "--load=/data/checkpoint"
                - "--data-path=/data/my-gpt2_text_document"
                - "--vocab-file=/data/roberta-large-mnli-vocab.json"
                - "--merge-file=/data/roberta-large-mnli-merges.txt"
                - "--data-impl=mmap"
                - "--split=949,50,1"
                - "--distributed-backend=nccl"
                - "--lr=1.5e-4"
                - "--lr-decay-style=cosine"
                - "--min-lr=1.0e-5"
                - "--weight-decay=1e-2"
                - "--clip-grad=1.0"
                - "--warmup=0.01"
                - "--checkpoint-activations"
                - "--log-interval=1"
                - "--save-interval=500"
                - "--eval-interval=100"
                - "--eval-iters=10"
                - "--fp16"
                - "--deepspeed"
                - "--deepspeed_config=/workspace/external/deepspeed/DeepSpeedExamples/Megatron-LM-v1.1.5-3D_parallelism/examples/ds_config.json"
                - "--zero-stage=0"
                - "--zero-reduce-bucket-size=50000000"
                - "--zero-allgather-bucket-size=5000000000"
              resources:
                limits:
                  nvidia.com/gpu: 1
              volumeMounts:
                - mountPath: /data
                  name: data
                  subPath: data
          volumes:
            - name: data
              persistentVolumeClaim:
                claimName: efs-a-claim
